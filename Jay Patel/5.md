What is the difference between bagging and boosting?

Bagging and boosting are both ensemble learning techniques, but they differ in how they combine multiple models and how they handle errors.

Bagging (Bootstrap Aggregating):

Trains multiple models independently on different random subsets of the training data (sampled with replacement).

Each model has equal weight in the final prediction, usually combined by averaging (for regression) or majority voting (for classification).

Reduces variance and helps prevent overfitting.

Example: Random Forest, where multiple decision trees are trained on different subsets of data.

Boosting:

Trains multiple models sequentially, where each new model focuses on the errors made by previous models.

Assigns higher weight to misclassified or poorly predicted samples, making the next model try harder to correct them.

Reduces bias and improves accuracy, but can be more prone to overfitting if not regularized.

Examples: AdaBoost, Gradient Boosting, XGBoost.

In short, bagging reduces variance by averaging independent models, while boosting reduces bias by sequentially correcting errors. Together, they are powerful tools but address different challenges in model performance.